**Project: Simulated Social Credit System (Technical Exercise)**

**Focus:** Data integration from diverse sources, real-time scoring updates, distributed data management, and reporting.

**Distributed Nature:**

*   **Data Ingestion:** Simulated data (financial transactions, "social behavior" events, online activity) generated by Go applications. **Redpanda** acts as the central, high-volume, distributed message bus for these event streams.
*   **Scoring Engine:** The scoring logic is implemented as a distributed stream processing application using **PySpark Structured Streaming**. PySpark consumes events from Redpanda, applies rules, and updates scores in real-time.
*   **Data Storage:** A polyglot persistence approach using multiple specialized databases to handle diverse data types, query patterns, and scalability requirements.
*   **Reporting & Access:** APIs and dashboards provide access to simulated scores, trends, and underlying data, requiring a distributed architecture.

**Database & Component Roles:**

1.  **Core Profile & Configuration Data (MongoDB):**
    *   **`citizens` collection:** Stores core citizen profiles (unique `nationalID`, demographic data, contact info, consent flags) and the *current* `scoreSnapshot` (score, tier, last calculated date). This is the primary source for citizen identity.
    *   **`scoring_rule_definitions` collection:** Contains detailed definitions of individual scoring rules (ID, conditions, points, multipliers, cooldowns, status, validity).
    *   **`system_configuration` collection:** Holds the active global configuration for the scoring system (baseline score, tier definitions, active rule set, score decay policy).
    *   *Role: MongoDB serves as the system of record for citizen identity, rule logic, and system settings. It provides quick lookups for current scores and configurations needed by PySpark.*

2.  **Social Graph & Relationships (Dgraph):**
    *   **`CitizenNode` type:** Stores `nationalID` as the primary ID and focuses purely on relationships between citizens (e.g., `family`, `colleagues`, `friends`). Minimal other attributes directly on nodes.
    *   *Role: Dgraph is dedicated to managing and querying complex relationships within the social graph. Analytics on this graph (e.g., deriving association risk) would be performed by separate PySpark jobs, with results potentially fed back to other systems if needed.*

3.  **Time-Series Event Data & Real-time Metrics (InfluxDB 2.x/3.x):**
    *   **`raw_events` Bucket:** Stores all incoming JSON-formatted `citizen_event` data from PySpark with a relatively short retention (e.g., 30-90 days). This includes `eventID`, `citizenID`, `eventType`, `eventSubtype`, `sourceSystem`, location data, and key-value pairs extracted from the JSON `payload` into queryable tags and fields. A full JSON of the payload can also be stored.
    *   **`derived_metrics` Bucket:**
        *   `score_change_history` measurement: Detailed log of every score change (old/new score, reason, triggering rule).
        *   Time-windowed aggregates (e.g., `citizen_activity_hourly`, `citizen_summary_daily`, `system_aggregates_daily`) generated by PySpark batch jobs from raw events.
        *   Potentially `social_graph_derived_metrics` pushed from Dgraph analysis jobs for trending.
    *   *Role: InfluxDB is the primary store for all time-series data. It captures raw event streams for detailed analysis and audit, tracks score history meticulously, and powers real-time monitoring and trend analysis through its query capabilities.*

4.  **Long-Term Event Archive & Batch Aggregates (ScyllaDB):**
    *   **`events_archive` table:** Provides long-term (e.g., 5+ years with TTL) storage of processed events in a queryable columnar format, optimized for batch analytics and audit. Partitioned by `citizen_id` and time.
    *   **Aggregate tables (e.g., `credit_score_aggregates_by_region_date`, `events_summary_by_type_location_date`):** Stores results of large-scale go-streams batch aggregation jobs for BI, reporting, and complex analytics. These tables are designed with specific query patterns in mind using CQL.
    *   **Potential Operational Role:** Could manage distributed state for rule cooldowns/frequency capping via a table like `rule_application_state`, providing fast lookups for the go-streams processing engine during event processing.
    *   *Role: ScyllaDB handles large-scale data warehousing needs: durable long-term event archival and serving pre-calculated, query-optimized aggregates for analytics. Its potential operational role for state management leverages its low-latency read/write capabilities.*

5.  **Analytical Query Engine (DuckDB):**
    *   **In-Memory Analytics:** DuckDB provides fast OLAP capabilities for complex analytical queries, aggregations, and ad-hoc data exploration.
    *   **Data Lake Integration:** Can directly query Parquet files exported from ScyllaDB or InfluxDB for historical analysis without data movement.
    *   **Cross-Database Analytics:** Supports federated queries across multiple data sources (MongoDB, InfluxDB, ScyllaDB) through extensions and connectors.
    *   **Reporting & BI:** Powers complex analytical reports, trend analysis, and data science workloads with SQL-based interface.
    *   **Go Integration:** Embedded directly in Go applications for real-time analytical capabilities or as a separate analytical service.
    *   *Role: DuckDB serves as the analytical powerhouse for complex queries, data science workloads, and business intelligence that require OLAP capabilities beyond what the operational databases provide efficiently.*

6.  **Event Streaming & Processing Backbone:**
    *   **Redpanda:** The central nervous system for data in motion. It decouples event producers from consumers, providing a resilient and scalable buffer for all event streams. Uses JSON format for events.
    *   **Go Streams (go-streams library):** The core real-time engine for stream processing, providing lightweight and efficient stream processing in Go.
        *   Consumes JSON events from Redpanda using Kafka source connector.
        *   Enriches events with data from MongoDB (rules, citizen flags if needed for multipliers).
        *   Applies scoring logic using composable flow operations (Map, Filter, Batch, etc.).
        *   Includes consulting ScyllaDB for rule cooldown states if implemented.
        *   Updates `scoreSnapshot` in MongoDB using sink connectors.
        *   Writes raw events and `score_change_history` to InfluxDB using dedicated sinks.
        *   Forwards processed events or derived data to ScyllaDB's `events_archive` using batch operations.
        *   May trigger Dgraph updates through dedicated flow pipelines or sink operations.

7.  **Event Generation, Batch Processing & APIs (Go Applications / Other Tools):**
    *   **Go Event Producer:** Generates simulated event data and publishes to Redpanda.
    *   **Go Batch Processing (using go-streams):**
        *   Performs large-scale aggregations on data from ScyllaDB (`events_archive`) or InfluxDB using batch flow operations.
        *   Writes results to ScyllaDB aggregate tables using bulk sink operations.
        *   Conducts periodic graph analysis on Dgraph data using specialized flow pipelines.
        *   Leverages go-streams' windowing operations (TumblingWindow, SlidingWindow) for time-based aggregations.
    *   **Go APIs:** Provide the access layer for external systems or dashboards to query data from all underlying databases.
    *   **DuckDB Analytics Service:** Dedicated service for complex analytical queries, cross-database joins, and data science workloads.

**Simplified Workflow Example:**

1.  **Simulated Event:** A Go script (event producer) generates a simulated "late payment" JSON event for a citizen (identified by their `nationalID`).
2.  **Redpanda Ingestion:** The event is published to a Redpanda topic (e.g.,`citizen_events`).
3.  **Go Streams Processing:** A go-streams application consumes the event:
    *   It fetches the relevant `scoring_rule_definitions` and current `system_configuration` (including tier definitions) from **MongoDB** using enrichment flows.
    *   *(Optional: If rule involves cooldowns)* It queries **ScyllaDB** (`rule_application_state`) to check if the rule can be applied for this citizen using lookup operations.
    *   It applies the rule logic to the event data (including the JSON `payload`) using Map and Filter flow operations.
    *   It updates the citizen's `scoreSnapshot` (current score, tier, last calculated) in **MongoDB** using dedicated sink operations.
    *   It writes the raw event (with extracted payload fields) to the `raw_events` bucket in **InfluxDB** using parallel sink operations.
    *   It writes a `score_change_history` record to the `derived_metrics` bucket in **InfluxDB** using time-series sink operations.
    *   It sends the processed event to be archived in **ScyllaDB**'s `events_archive` table using batch sink operations.
    *   *(Optional: If cooldowns are used)* It updates the `rule_application_state` in **ScyllaDB** using atomic update operations.
4.  **Batch Processing:** Later, go-streams batch jobs run using TumblingWindow operations to generate hourly/daily summaries from raw events (e.g., `citizen_activity_hourly`).
5.  **Batch Aggregation:** Periodically, go-streams batch jobs read from ScyllaDB's `events_archive` and/or InfluxDB's `derived_metrics` using source connectors to create broad analytical aggregates in ScyllaDB tables (e.g., `credit_score_aggregates_by_region_date`) using Reduce and Fold operations.
6.  **Reporting:** A separate API service (e.g., Go) queries the appropriate databases to serve data:
    *   **MongoDB** for current citizen profiles and scores.
    *   **InfluxDB** for recent event history, score trends, and real-time metrics.
    *   **ScyllaDB** for long-term archival queries and broad analytical reports.
    *   **Dgraph** for displaying social connections.
    *   **DuckDB** for complex analytical queries, cross-database analytics, trend analysis, and data science workloads.

**(Object Storage like MinIO/S3 would still be relevant for database backups, Terraform state, etc., but is not a primary part of the active data flow for scoring).**
