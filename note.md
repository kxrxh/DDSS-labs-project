**Project: Simulated Social Credit System (Technical Exercise)**

**Focus:** Data integration from diverse sources, real-time scoring updates, distributed data management, and reporting.

**Distributed Nature:**

*   **Data Ingestion:** Simulated data (financial transactions, "social behavior" events, online activity) generated by Go applications. **Redpanda** acts as the central, high-volume, distributed message bus for these event streams.
*   **Scoring Engine:** The scoring logic is implemented as a distributed stream processing application using **PySpark Structured Streaming**. PySpark consumes events from Redpanda, applies rules, and updates scores in real-time.
*   **Data Storage:** A polyglot persistence approach using multiple specialized databases to handle diverse data types, query patterns, and scalability requirements.
*   **Reporting & Access:** APIs and dashboards provide access to simulated scores, trends, and underlying data, requiring a distributed architecture.

**Database & Component Roles:**

1.  **Core Profile & Configuration Data (MongoDB):**
    *   **`citizens` collection:** Stores core citizen profiles (unique `nationalID`, demographic data, contact info, consent flags) and the *current* `scoreSnapshot` (score, tier, last calculated date). This is the primary source for citizen identity.
    *   **`scoring_rule_definitions` collection:** Contains detailed definitions of individual scoring rules (ID, conditions, points, multipliers, cooldowns, status, validity).
    *   **`system_configuration` collection:** Holds the active global configuration for the scoring system (baseline score, tier definitions, active rule set, score decay policy).
    *   *Role: MongoDB serves as the system of record for citizen identity, rule logic, and system settings. It provides quick lookups for current scores and configurations needed by PySpark.*

2.  **Social Graph & Relationships (Dgraph):**
    *   **`CitizenNode` type:** Stores `nationalID` as the primary ID and focuses purely on relationships between citizens (e.g., `family`, `colleagues`, `friends`). Minimal other attributes directly on nodes.
    *   *Role: Dgraph is dedicated to managing and querying complex relationships within the social graph. Analytics on this graph (e.g., deriving association risk) would be performed by separate PySpark jobs, with results potentially fed back to other systems if needed.*

3.  **Time-Series Event Data, Real-time Metrics & Analytics (InfluxDB 2.x/3.x):**
    *   **`raw_events` Bucket:** Stores all incoming JSON-formatted `citizen_event` data from go-streams with a relatively short retention (e.g., 30-90 days). This includes `eventID`, `citizenID`, `eventType`, `eventSubtype`, `sourceSystem`, location data, and key-value pairs extracted from the JSON `payload` into queryable tags and fields. A full JSON of the payload can also be stored.
    *   **`derived_metrics` Bucket:**
        *   `score_change_history` measurement: Detailed log of every score change (old/new score, reason, triggering rule).
        *   Time-windowed aggregates (e.g., `citizen_activity_hourly`, `citizen_summary_daily`, `system_aggregates_daily`) generated by go-streams batch jobs from raw events.
        *   Potentially `social_graph_derived_metrics` pushed from Dgraph analysis jobs for trending.
    *   **`analytical_datasets` Bucket:**
        *   Complex analytical aggregations using Flux query language for OLAP operations.
        *   Cross-citizen comparative analysis and behavioral pattern detection.
        *   Statistical summaries and trend analysis for reporting and dashboards.
        *   Real-time analytical queries for complex business intelligence needs.
    *   *Role: InfluxDB serves as both the primary time-series data store and the analytical engine. It captures raw event streams, tracks score history, powers real-time monitoring, and provides sophisticated analytical capabilities through Flux queries for complex trend analysis, statistical operations, and business intelligence.*

4.  **Long-Term Event Archive & Batch Aggregates (ScyllaDB):**
    *   **`events_archive` table:** Provides long-term (e.g., 5+ years with TTL) storage of processed events in a queryable columnar format, optimized for batch analytics and audit. Partitioned by `citizen_id` and time.
    *   **Aggregate tables (e.g., `credit_score_aggregates_by_region_date`, `events_summary_by_type_location_date`):** Stores results of large-scale go-streams batch aggregation jobs for BI, reporting, and complex analytics. These tables are designed with specific query patterns in mind using CQL.
    *   **Potential Operational Role:** Could manage distributed state for rule cooldowns/frequency capping via a table like `rule_application_state`, providing fast lookups for the go-streams processing engine during event processing.
    *   *Role: ScyllaDB handles large-scale data warehousing needs: durable long-term event archival and serving pre-calculated, query-optimized aggregates for analytics. Its potential operational role for state management leverages its low-latency read/write capabilities.*

5.  **Event Streaming & Processing Backbone:**
    *   **Redpanda:** The central nervous system for data in motion. It decouples event producers from consumers, providing a resilient and scalable buffer for all event streams. Uses JSON format for events.
    *   **Go Streams (go-streams library):** The core real-time engine for stream processing, providing lightweight and efficient stream processing in Go.
        *   Consumes JSON events from Redpanda using Kafka source connector.
        *   Enriches events with data from MongoDB (rules, citizen flags if needed for multipliers).
        *   Applies scoring logic using composable flow operations (Map, Filter, Batch, etc.).
        *   Includes consulting ScyllaDB for rule cooldown states if implemented.
        *   Updates `scoreSnapshot` in MongoDB using sink connectors.
        *   Writes raw events and `score_change_history` to InfluxDB using dedicated sinks.
        *   Forwards processed events or derived data to ScyllaDB's `events_archive` using batch operations.
        *   May trigger Dgraph updates through dedicated flow pipelines or sink operations.

6.  **Event Generation, Batch Processing & APIs (Go Applications):**
    *   **Go Event Producer:** Generates simulated event data and publishes to Redpanda.
    *   **Go Batch Processing (using go-streams):**
        *   Performs large-scale aggregations on data from ScyllaDB (`events_archive`) or InfluxDB using batch flow operations.
        *   Writes results to ScyllaDB aggregate tables and InfluxDB analytical datasets using bulk sink operations.
        *   Conducts periodic graph analysis on Dgraph data using specialized flow pipelines.
        *   Leverages go-streams' windowing operations (TumblingWindow, SlidingWindow) for time-based aggregations.
    *   **Go APIs:** Provide the access layer for external systems or dashboards to query data from all underlying databases and coordinate cross-database analytics.

**Simplified Workflow Example:**

1.  **Simulated Event:** A Go script (event producer) generates a simulated "late payment" JSON event for a citizen (identified by their `nationalID`).
2.  **Redpanda Ingestion:** The event is published to a Redpanda topic (e.g.,`citizen_events`).
3.  **Go Streams Processing:** A go-streams application consumes the event:
    *   It fetches the relevant `scoring_rule_definitions` and current `system_configuration` (including tier definitions) from **MongoDB** using enrichment flows.
    *   *(Optional: If rule involves cooldowns)* It queries **ScyllaDB** (`rule_application_state`) to check if the rule can be applied for this citizen using lookup operations.
    *   It applies the rule logic to the event data (including the JSON `payload`) using Map and Filter flow operations.
    *   It updates the citizen's `scoreSnapshot` (current score, tier, last calculated) in **MongoDB** using dedicated sink operations.
    *   It writes the raw event (with extracted payload fields) to the `raw_events` bucket in **InfluxDB** using parallel sink operations.
    *   It writes a `score_change_history` record to the `derived_metrics` bucket in **InfluxDB** using time-series sink operations.
    *   It sends the processed event to be archived in **ScyllaDB**'s `events_archive` table using batch sink operations.
    *   *(Optional: If cooldowns are used)* It updates the `rule_application_state` in **ScyllaDB** using atomic update operations.
4.  **Batch Processing:** Later, go-streams batch jobs run using TumblingWindow operations to generate hourly/daily summaries from raw events (e.g., `citizen_activity_hourly`).
5.  **Batch Aggregation:** Periodically, go-streams batch jobs read from ScyllaDB's `events_archive` and/or InfluxDB's `derived_metrics` using source connectors to create broad analytical aggregates in both ScyllaDB tables (e.g., `credit_score_aggregates_by_region_date`) and InfluxDB analytical datasets using Reduce and Fold operations.
6.  **Reporting:** A separate API service (e.g., Go) queries the appropriate databases to serve data:
    *   **MongoDB** for current citizen profiles and scores.
    *   **InfluxDB** for recent event history, score trends, real-time metrics, and complex analytical queries.
    *   **ScyllaDB** for long-term archival queries and broad analytical reports.
    *   **Dgraph** for displaying social connections.
    *   **Go APIs** coordinate cross-database analytics and provide unified analytical capabilities.

**(Object Storage like MinIO/S3 would still be relevant for database backups, Terraform state, etc., but is not a primary part of the active data flow for scoring).**
