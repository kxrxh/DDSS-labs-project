# social-rating-job.yaml
apiVersion: flink.apache.org/v1beta1
kind: FlinkDeployment
metadata:
  name: social-rating-job
  namespace: dev-flink-jobs # Target namespace watched by the operator
spec:
  # Use a standard Flink image matching your Flink version
  image: flink:1.20.1-scala_2.12-java17 # Updated to Java 17 image with patch version
  flinkVersion: v1_20 # Matches the base image and pom.xml Flink version
  flinkConfiguration:
    taskmanager.numberOfTaskSlots: "2"
    # Increase checkpoint interval if needed
    execution.checkpointing.interval: "1min"
    # Add any other Flink configurations your job needs
    # e.g., environment variables for connection strings, parallelism defaults
    # Refer to: https://nightlies.apache.org/flink/flink-docs-release-1.20/docs/deployment/config/

  serviceAccount: flink # Default service account, ensure it exists or create one

  podTemplate:
    spec:
      # Define a hostPath volume pointing to a directory on your local machine
      volumes:
      - name: flink-job-jar-hostpath
        hostPath:
          # IMPORTANT: This path must exist on your host machine where Kubernetes node runs.
          # Ensure Kubernetes has permissions to read from this path.
          # You will need to copy your JAR file into this directory on your host.
          path: /tmp/flink-jars # Example path, change if needed
          type: DirectoryOrCreate # Creates the directory if it doesn't exist (on the node)

      # Mount the hostPath volume into the main Flink containers
      containers:
      - name: flink-main-container # Name must be this for the operator to merge mounts
        volumeMounts:
        - name: flink-job-jar-hostpath
          mountPath: /opt/flink/usrlib # Mount into Flink's standard user lib dir
          # readOnly: true # Optional: mount as read-only

  jobManager:
    resource:
      memory: "1024m"
      cpu: 1
    podTemplate:
      spec:
        # Define a hostPath volume pointing to a directory on your local machine
        volumes:
        - name: flink-job-jar-hostpath
          hostPath:
            # IMPORTANT: This path must exist on your host machine where Kubernetes node runs.
            # Ensure Kubernetes has permissions to read from this path.
            # You will need to copy your JAR file into this directory on your host.
            path: /tmp/flink-jars # Example path, change if needed
            type: DirectoryOrCreate # Creates the directory if it doesn't exist (on the node)
        
        # Add initContainer to create Kafka topics
        initContainers:
        - name: kafka-topic-creator
          image: bitnami/kafka:latest # Image with kafka-topics.sh
          # Override entrypoint to directly run commands
          command: ['/bin/sh', '-c']
          # Command to create topics if they don't exist
          # Adjust partitions/replication factor if needed
          args:
            - |
              echo "Waiting for Kafka broker...";
              until kafka-topics.sh --bootstrap-server dev-redpanda.dev-streaming.svc.cluster.local:9093 --list > /dev/null 2>&1; do 
                echo -n .; 
                sleep 1; 
              done; 
              echo "Kafka available.";
              echo "Creating topic input_events..."; 
              kafka-topics.sh --bootstrap-server dev-redpanda.dev-streaming.svc.cluster.local:9093 --create --if-not-exists --topic input_events --partitions 1 --replication-factor 1; 
              echo "Creating topic secondary_updates..."; 
              kafka-topics.sh --bootstrap-server dev-redpanda.dev-streaming.svc.cluster.local:9093 --create --if-not-exists --topic secondary_updates --partitions 1 --replication-factor 1;
              echo "Topic creation commands executed.";
          
        # Mount the hostPath volume into the main Flink containers
        containers:
        - name: flink-main-container # Name must be this for the operator to merge mounts
          volumeMounts:
          - name: flink-job-jar-hostpath
            mountPath: /opt/flink/usrlib # Mount into Flink's standard user lib dir
            # readOnly: true # Optional: mount as read-only

  taskManager:
    resource:
      memory: "1536m" # Slightly more memory for task managers
      cpu: 1
    # Add podTemplate for TaskManager if env vars are needed here too
    podTemplate:
      spec:
        containers:
        - name: flink-main-container # Name must match
          # REMOVE environment variables from here (should inherit from top-level podTemplate)

  job:
    # Points to the JAR file mounted from the host via hostPath
    # Ensure the filename here matches the filename you copied to the host path
    jarURI: local:///opt/flink/usrlib/social-rating-flink-job.jar 
    entryClass: "com.github.kxrxh.DataStreamJob" # Main class from pom.xml
    args: [
      # Kafka/Redpanda Configuration (Consolidated)
      # Verify these broker addresses and topic names
      "--kafka.brokers", "dev-redpanda.dev-streaming.svc.cluster.local:9093", # Changed to service address
      "--kafka.input.brokers", "dev-redpanda.dev-streaming.svc.cluster.local:9093", # Changed to service address
      "--kafka.output.brokers", "dev-redpanda.dev-streaming.svc.cluster.local:9093", # Changed to service address
      "--kafka.input.topic", "input_events", 
      "--kafka.consumer.group", "social-rating-job-consumer",
      "--kafka.secondary.update.topic", "secondary_updates",
      
      # MongoDB Configuration (Consolidated)
      # Verify DB name and collection names
      "--mongo.uri", "mongodb://mongodb-service.dev-db.svc.cluster.local:27017",
      "--mongo.db.name", "social_rating", 
      "--mongo.polling.interval.seconds", "60",
      "--mongo.rules.collection", "scoring_rules",
      "--mongo.snapshots.collection", "citizen_snapshots",
      "--mongo.timestamps.collection", "citizen_first_seen",

      # Dgraph Configuration (Consolidated)
      # Verify correct Dgraph address/URI - There were two different ones specified previously
      # Using --dgraph.address value: "dgraph-alpha.dev-db.svc.cluster.local:9080"
      "--dgraph.uri", "dgraph-alpha.dev-db.svc.cluster.local:9080", # <-- Verify this hostname/port
      
      # InfluxDB Configuration (Consolidated)
      # Verify URL, org, and bucket
      "--influxdb.url", "http://dev-influxdb-influxdb2.dev-analytics.svc.cluster.local:80",
      "--influxdb.token", "my-super-secret-auth-token", # Using specific token from later args
      "--influxdb.org", "social-rating-org",           # Using specific org from later args
      "--influxdb.bucket", "social-rating-bucket",     # Using specific bucket from later args
      
      # ClickHouse Configuration 
      # FIXME: Verify this JDBC URL. The hostname caused UnknownHostException in logs.
      # Check the correct Kubernetes service name and namespace for ClickHouse.
      "--clickhouse.jdbc.url", "jdbc:clickhouse://clickhouse-dev-clickhouse.dev-analytics.svc.cluster.local:8123", 
      "--clickhouse.user", "default",
      "--clickhouse.password", "SomeStrongPassword", # Added placeholder password
      
      # Async Defaults 
      "--async.enrich.demographics.timeout.ms", "5000",
      "--async.enrich.demographics.capacity", "100",
      "--async.enrich.relation.timeout.ms", "5000",
      "--async.enrich.relation.capacity", "100",
      "--async.related.effect.timeout.ms", "5000",
      "--async.related.effect.capacity", "50",
      
      # Enrichment Cache Config
      "--enrichment.demographics.cache.ttl.seconds", "300",
      "--enrichment.relations.cache.ttl.seconds", "60",

      # Job Configuration
      "-Dlog4j.configurationFile=file:/opt/flink/conf/log4j-console.properties",
      "--job.name", "Social Rating Flink Job"
      # Removed duplicated/conflicting arguments below this line
    ]
    parallelism: 2 # Set desired parallelism
    upgradeMode: stateless # Use 'savepoint' if your job maintains state
    state: running # Start the job automatically 